From 494a7db71ec5f9787592219117940cb683aba2ab Mon Sep 17 00:00:00 2001
From: David Kahurani <redress@duck.com>
Date: Fri, 9 May 2025 15:02:56 +0300
Subject: [PATCH] io_uring: Each process has it's own unique file descriptors                                                              

 Each task has it's own unique file descriptors which means that the
 list ctx->tctx_list can only ever get one item long Should it be a list?
 Probably not

Signed-off-by: David Kahurani <redress@duck.com>
---
 include/linux/io_uring_types.h |  2 +-
 io_uring/cancel.c              |  8 +++++++-
 io_uring/io_uring.c            | 23 ++++++++++++++++-------
 io_uring/register.c            | 12 +++++++++---
 io_uring/tctx.c                |  6 ++----
 5 files changed, 35 insertions(+), 16 deletions(-)

diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 1f0caf432..5f832b103 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -399,7 +399,7 @@ struct io_ring_ctx {
 	struct llist_head		fallback_llist;
 	struct delayed_work		fallback_work;
 	struct work_struct		exit_work;
-	struct list_head                tctx_list;
+	struct io_tctx_node             *tctx_node;
 	struct completion		ref_comp;
 
 	/* io-wq management, e.g. thread count */
diff --git a/io_uring/cancel.c b/io_uring/cancel.c
index 484193567..3bb044a64 100644
--- a/io_uring/cancel.c
+++ b/io_uring/cancel.c
@@ -183,14 +183,20 @@ static int __io_async_cancel(struct io_cancel_data *cd,
 	/* slow path, try all io-wq's */
 	io_ring_submit_lock(ctx, issue_flags);
 	ret = -ENOENT;
-	list_for_each_entry(node, &ctx->tctx_list, ctx_node) {
+
+	node = ctx->tctx_node;
+
+	if (node) {
+
 		ret = io_async_cancel_one(node->task->io_uring, cd);
+
 		if (ret != -ENOENT) {
 			if (!all)
 				break;
 			nr++;
 		}
 	}
+
 	io_ring_submit_unlock(ctx, issue_flags);
 	return all ? nr : ret;
 }
diff --git a/io_uring/io_uring.c b/io_uring/io_uring.c
index 24b9e9a51..8896fed4d 100644
--- a/io_uring/io_uring.c
+++ b/io_uring/io_uring.c
@@ -343,7 +343,7 @@ static __cold struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 	INIT_LIST_HEAD(&ctx->timeout_list);
 	INIT_LIST_HEAD(&ctx->ltimeout_list);
 	init_llist_head(&ctx->work_llist);
-	INIT_LIST_HEAD(&ctx->tctx_list);
+	ctx->tctx_node = NULL;
 	ctx->submit_state.free_list.next = NULL;
 	INIT_HLIST_HEAD(&ctx->waitid_list);
 #ifdef CONFIG_FUTEX
@@ -2911,14 +2911,14 @@ static __cold void io_ring_exit_work(struct work_struct *work)
 	exit.ctx = ctx;
 
 	mutex_lock(&ctx->uring_lock);
-	while (!list_empty(&ctx->tctx_list)) {
+	node = ctx->tctx_node;
+
+	if (node) {
+
 		WARN_ON_ONCE(time_after(jiffies, timeout));
 
-		node = list_first_entry(&ctx->tctx_list, struct io_tctx_node,
-					ctx_node);
-		/* don't spin on a single task if cancellation failed */
-		list_rotate_left(&ctx->tctx_list);
 		ret = task_work_add(node->task, &exit.task_work, TWA_SIGNAL);
+
 		if (WARN_ON_ONCE(ret))
 			continue;
 
@@ -2928,9 +2928,12 @@ static __cold void io_ring_exit_work(struct work_struct *work)
 		 * wait_for_completion_interruptible_timeout() on why this
 		 * wait is marked as interruptible.
 		 */
+
 		wait_for_completion_interruptible(&exit.completion);
+
 		mutex_lock(&ctx->uring_lock);
 	}
+
 	mutex_unlock(&ctx->uring_lock);
 	spin_lock(&ctx->completion_lock);
 	spin_unlock(&ctx->completion_lock);
@@ -3021,18 +3024,24 @@ static __cold bool io_uring_try_cancel_iowq(struct io_ring_ctx *ctx)
 	bool ret = false;
 
 	mutex_lock(&ctx->uring_lock);
-	list_for_each_entry(node, &ctx->tctx_list, ctx_node) {
+	node = ctx->tctx_node;
+
+	if (node) {
 		struct io_uring_task *tctx = node->task->io_uring;
 
 		/*
 		 * io_wq will stay alive while we hold uring_lock, because it's
 		 * killed after ctx nodes, which requires to take the lock.
 		 */
+
 		if (!tctx || !tctx->io_wq)
 			continue;
+
 		cret = io_wq_cancel_cb(tctx->io_wq, io_cancel_ctx_cb, ctx, true);
+
 		ret |= (cret != IO_WQ_CANCEL_NOTFOUND);
 	}
+
 	mutex_unlock(&ctx->uring_lock);
 
 	return ret;
diff --git a/io_uring/register.c b/io_uring/register.c
index 9a4d2fbce..cbce6353f 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -318,15 +318,21 @@ static __cold int io_register_iowq_max_workers(struct io_ring_ctx *ctx,
 		return 0;
 
 	/* now propagate the restriction to all registered users */
-	list_for_each_entry(node, &ctx->tctx_list, ctx_node) {
+	node = ctx->tctx_node;
+
+	if (node) {
+
 		tctx = node->task->io_uring;
-		if (WARN_ON_ONCE(!tctx->io_wq))
-			continue;
+
+		if (!tctx->io_wq)
+			return 0;
 
 		for (i = 0; i < ARRAY_SIZE(new_count); i++)
 			new_count[i] = ctx->iowq_limits[i];
+
 		/* ignore errors, it always returns zero anyway */
 		(void)io_wq_max_workers(tctx->io_wq, new_count);
+
 	}
 	return 0;
 err:
diff --git a/io_uring/tctx.c b/io_uring/tctx.c
index 8429356bb..59c8d63ef 100644
--- a/io_uring/tctx.c
+++ b/io_uring/tctx.c
@@ -144,9 +144,8 @@ int __io_uring_add_tctx_node(struct io_ring_ctx *ctx)
 			return ret;
 		}
 
-		count++;
 		mutex_lock(&ctx->uring_lock);
-		list_add(&node->ctx_node, &ctx->tctx_list);
+		ctx->tctx_node = node;
 		mutex_unlock(&ctx->uring_lock);
 	}
 	return 0;
@@ -183,10 +182,9 @@ __cold void io_uring_del_tctx_node(unsigned long index)
 		return;
 
 	WARN_ON_ONCE(current != node->task);
-	WARN_ON_ONCE(list_empty(&node->ctx_node));
 
 	mutex_lock(&node->ctx->uring_lock);
-	list_del(&node->ctx_node);
+	node->ctx->tctx_node = NULL;
 	mutex_unlock(&node->ctx->uring_lock);
 
 	if (tctx->last == node->ctx)
-- 
2.43.0

